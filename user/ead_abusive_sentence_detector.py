# -*- coding: utf-8 -*-
"""EAD_abusive_sentence_detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12TvWJbXPLIGet_pIzj3taGwwCO8K0-Wh
"""

# !pip install tweet-preprocessor

import preprocessor as p
import nltk
from nltk.corpus import stopwords
import pandas as pd
import numpy as np
import re
import string
import pickle
from tensorflow import keras
from keras.models import load_model
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import os
# nltk.download('stopwords')
# nltk.download('punkt')


def clean_tweets(tweet):
    # HappyEmoticons
    emoticons_happy = set([
        ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',
        ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',
        '=-3', '=3', ':-))', ":'-)", ":')", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',
        'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',
        '<3'
    ])
    # Sad Emoticons
    emoticons_sad = set([
        ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',
        ':-[', ':-<', '=\\', '=/', '>:(', ':(', '>.<', ":'-(", ":'(", ':\\', ':-c',
        ':c', ':{', '>:\\', ';('
    ])

    emoticons = emoticons_happy.union(emoticons_sad)

    # Emoji patterns
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)

    tweet = tweet.replace("#","")
    stop_words = set(stopwords.words('english'))

#after tweepy preprocessing the colon symbol left remain after      #removing mentions
    tweet = re.sub(r':', '', tweet)
    tweet = re.sub(r'‚Ä¶', '', tweet)
#replace consecutive non-ASCII characters with a space
    tweet = re.sub(r'[^\x00-\x7F]+',' ', tweet)
#remove emojis from tweet
    tweet = emoji_pattern.sub(r'', tweet)

    word_tokens = nltk.tokenize.word_tokenize(tweet)
#filter using NLTK library append it to a string
    filtered_tweet = [w for w in word_tokens if not w in stop_words]
    # filtered_tweet = [w for w in word_tokens]
    filtered_tweet = []
#looping through conditions
    for w in word_tokens:
#check tokens against stop words , emoticons and punctuations
        if w not in stop_words and w not in emoticons and w not in string.punctuation:

            filtered_tweet.append(w)
    return ' '.join(filtered_tweet).lower()



def predict(sentence,model,classes,tokenizer):
  sentence = clean_tweets(p.clean(sentence))
  twt = [sentence]
  twt = tokenizer.texts_to_sequences(twt)
  twt = pad_sequences(twt, maxlen=25, dtype='int32', value=0)
  sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]
  return np.argmax(sentiment)

# sentence = 'good morning, how are You doing today'


def abusive_detect_main(sentence):
    # Load Tokenizer
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    path=os.path.join(BASE_DIR,'static')
    path=os.path.join(path,'threat')

    print(path)
    with open(path + '/' + 'final_balenced_v2_tokenizer.pickle', 'rb') as handle:
        tokenizer = pickle.load(handle)

    # Load Model
    model = load_model(path + '/' +'final_balenced_v2.h5')


    classes = ['normal', 'hateful', 'spam', 'sexism', 'abusive']

    return predict(sentence,model,classes,tokenizer)
# print(abusive_detect_main("hate you"))
